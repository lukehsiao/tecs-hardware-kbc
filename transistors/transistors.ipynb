{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Transistor Electrical Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we seek to extract\n",
    "\n",
    "- maximum storage tempurature\n",
    "- minimum storage tempurature\n",
    "- polarity\n",
    "- maximum collector emitter voltage\n",
    "- maximum emitter base voltage\n",
    "- maximum collector current\n",
    "- total device dissipation\n",
    "- minimum dc gain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# To allow importing from the general utils\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Configure logging for Fonduer\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    format=\"[%(levelname)s] %(name)s:%(lineno)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# See https://docs.python.org/3/library/os.html#os.cpu_count\n",
    "PARALLEL = len(os.sched_getaffinity(0))\n",
    "COMPONENT = \"transistors\"\n",
    "conn_string = \"postgresql://localhost:5432/\" + COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've run this before, set FIRST_TIME to False to save time\n",
    "FIRST_TIME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.meta:86 - Connecting user:None to localhost:5432/transistors\n",
      "[INFO] fonduer.meta:110 - Initializing the storage schema\n"
     ]
    }
   ],
   "source": [
    "from fonduer import Meta\n",
    "\n",
    "session = Meta.init(conn_string).Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] utils.utils:60 - Reloading pre-parsed dataset.\n",
      "[INFO] __main__:3 - # of train Documents: 100\n",
      "[INFO] __main__:4 - # of dev Documents: 100\n",
      "[INFO] __main__:5 - # of test Documents: 75\n"
     ]
    }
   ],
   "source": [
    "from utils import parse_dataset\n",
    "docs, train_docs, dev_docs, test_docs = parse_dataset(session, first_time=False, parallel=PARALLEL)\n",
    "logger.info(f\"# of train Documents: {len(train_docs)}\")\n",
    "logger.info(f\"# of dev Documents: {len(dev_docs)}\")\n",
    "logger.info(f\"# of test Documents: {len(test_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__:3 - Documents: 275\n",
      "[INFO] __main__:4 - Sections: 275\n",
      "[INFO] __main__:5 - Paragraphs: 136992\n",
      "[INFO] __main__:6 - Sentences: 141638\n",
      "[INFO] __main__:7 - Figures: 7440\n"
     ]
    }
   ],
   "source": [
    "from fonduer.parser.models import Document, Section, Paragraph, Sentence, Figure\n",
    "\n",
    "logger.info(f\"Documents: {session.query(Document).count()}\")\n",
    "logger.info(f\"Sections: {session.query(Section).count()}\")\n",
    "logger.info(f\"Paragraphs: {session.query(Paragraph).count()}\")\n",
    "logger.info(f\"Sentences: {session.query(Sentence).count()}\")\n",
    "logger.info(f\"Figures: {session.query(Figure).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Mention Extraction, Candidate Extraction Multimodal Featurization\n",
    "\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation\n",
    "candidates based on user-provided **matchers** and **throttlers**. Then,\n",
    "`Fonduer` leverages the multimodality information captured in the unified data\n",
    "model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Mention Extraction\n",
    "\n",
    "The first step is to extract **mentions** from our corpus. A `mention` is the\n",
    "type of object which makes up a `candidate`. For example, if we wanted to\n",
    "extract pairs of transistor part numbers and their corresponding maximum\n",
    "storage temperatures, the transistor part number would be one `mention` while\n",
    "the temperature value would be another. These `mention`s are then combined to\n",
    "create `candidates`, where our task is to predict which `candidates` are true\n",
    "in the associated document.\n",
    "\n",
    "We first start by defining and naming our two `mention`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import mention_subclass\n",
    "\n",
    "Part = mention_subclass(\"Part\")\n",
    "StgTempMin = mention_subclass(\"StgTempMin\")\n",
    "StgTempMax = mention_subclass(\"StgTempMax\")\n",
    "Polarity = mention_subclass(\"Polarity\")\n",
    "CeVMax = mention_subclass(\"CeVMax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transistor_matchers import get_matcher\n",
    "stg_temp_min_matcher = get_matcher(\"stg_temp_min\")\n",
    "stg_temp_max_matcher = get_matcher(\"stg_temp_max\")\n",
    "polarity_matcher = get_matcher(\"polarity\")\n",
    "ce_v_max_matcher = get_matcher(\"ce_v_max\")\n",
    "part_matcher = get_matcher(\"part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two matchers define each entity in our relation schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Mention's `MentionSpace`\n",
    "\n",
    "Next, in order to define the \"space\" of all mentions that are even considered\n",
    "from the document, we need to define a `MentionSpace` for each component of the\n",
    "relation we wish to extract. Fonduer provides a default `MentionSpace` for you\n",
    "to use, but you can also extend the default `MentionSpace` depending on your\n",
    "needs.\n",
    "\n",
    "In the case of transistor part numbers, the `MentionSpace` can be quite complex\n",
    "due to the need to handle implicit part numbers that are implied in text like\n",
    "\"BC546A/B/C...BC548A/B/C\", which refers to 9 unique part numbers. To handle\n",
    "these, we consider all n-grams up to 3 words long.\n",
    "\n",
    "In contrast, the `MentionSpace` for temperature values is simpler: we only need\n",
    "to process different Unicode representations of a (`-`), and don't need to look\n",
    "at more than two words at a time.\n",
    "\n",
    "When no special preprocessing like this is needed, we could have used the\n",
    "default `Ngrams` class provided by `fonduer`. For example, if we were looking\n",
    "to match polarities, which only take the form of \"NPN\" or \"PNP\", we could've\n",
    "used `ngrams = MentionNgrams(n_max=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionNgrams\n",
    "from transistor_spaces import MentionNgramsPart, MentionNgramsTemp, MentionNgramsVolt\n",
    "    \n",
    "part_ngrams = MentionNgramsPart(parts_by_doc=None, n_max=3)\n",
    "temp_ngrams = MentionNgramsTemp(n_max=2)\n",
    "volt_ngrams = MentionNgramsVolt(n_max=1)\n",
    "polarity_ngrams = MentionNgrams(n_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Mention Extraction \n",
    "\n",
    "Next, we create a `MentionExtractor` to extract the mentions from all of\n",
    "our documents based on the `MentionSpace` and matchers we defined above.\n",
    "\n",
    "View the API for the MentionExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/latest/user/candidates.html#fonduer.candidates.MentionExtractor).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates import MentionExtractor\n",
    "\n",
    "mention_extractor = MentionExtractor(\n",
    "    session,\n",
    "    [Part, StgTempMin, StgTempMax, Polarity, CeVMax],\n",
    "    [part_ngrams, temp_ngrams, temp_ngrams, polarity_ngrams, volt_ngrams],\n",
    "    [\n",
    "        part_matcher,\n",
    "        stg_temp_min_matcher,\n",
    "        stg_temp_max_matcher,\n",
    "        polarity_matcher,\n",
    "        ce_v_max_matcher,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we run the extractor on all of our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__:6 - Total Mentions: 21286\n",
      "[INFO] __main__:7 - Total Part: 5940\n",
      "[INFO] __main__:8 - Total StgTempMin: 3438\n",
      "[INFO] __main__:9 - Total StgTempMax: 3438\n",
      "[INFO] __main__:10 - Total Polarity: 1475\n",
      "[INFO] __main__:11 - Total CeVMax: 3557\n"
     ]
    }
   ],
   "source": [
    "from fonduer.candidates.models import Mention\n",
    "\n",
    "if FIRST_TIME and False:\n",
    "    mention_extractor.apply(docs, parallelism=PARALLEL)\n",
    "\n",
    "logger.info(f\"Total Mentions: {session.query(Mention).count()}\")\n",
    "logger.info(f\"Total Part: {session.query(Part).count()}\")\n",
    "logger.info(f\"Total StgTempMin: {session.query(StgTempMin).count()}\")\n",
    "logger.info(f\"Total StgTempMax: {session.query(StgTempMax).count()}\")\n",
    "logger.info(f\"Total Polarity: {session.query(Polarity).count()}\")\n",
    "logger.info(f\"Total CeVMax: {session.query(CeVMax).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Candidate Extraction\n",
    "\n",
    "Now that we have both defined and extracted the Mentions that can be used to compose Candidates, we are ready to move on to extracting Candidates. Like we did with the Mentions, we first define what each candidate schema looks like. In this example, we create a candidate that is composed of a `Part` and a `Temp` mention as we defined above. We name this candidate \"PartTemp\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.candidates.models import candidate_subclass\n",
    "\n",
    "PartStgTempMin = candidate_subclass(\"PartStgTempMin\", [Part, StgTempMin])\n",
    "PartStgTempMax = candidate_subclass(\"PartStgTempMax\", [Part, StgTempMax])\n",
    "PartPolarity = candidate_subclass(\"PartPolarity\", [Part, Polarity])\n",
    "PartCeVMax = candidate_subclass(\"PartCeVMax\", [Part, CeVMax])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate `Throttlers`\n",
    "\n",
    "Next, we need to define **throttlers**, which allow us to further prune excess candidates and avoid unnecessarily materializing invalid candidates. Throttlers, like matchers, act as hard filters, and should be created to have high precision while maintaining complete recall, if possible.\n",
    "\n",
    "Here, we create a throttler that discards candidates if they are in the same table, but the part and storage temperature are not vertically or horizontally aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transistor_throttlers import stg_temp_filter, polarity_filter, ce_v_max_filter\n",
    "\n",
    "temp_throttler = stg_temp_filter\n",
    "polarity_throttler = polarity_filter\n",
    "ce_v_max_throttler = ce_v_max_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the `CandidateExtractor`\n",
    "\n",
    "Now, we have all the component necessary to perform candidate extraction. We have defined the Mentions that compose each candidate and a throttler to prunes away excess candidates. We now can define the `CandidateExtractor` with the candidate subclass and corresponding throttler to use.\n",
    "\n",
    "View the API for the CandidateExtractor on [ReadTheDocs](https://fonduer.readthedocs.io/en/docstrings/user/candidates.html#fonduer.candidates.CandidateExtractor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(\n",
    "    session,\n",
    "    [PartStgTempMin, PartStgTempMax, PartPolarity, PartCeVMax],\n",
    "    throttlers=[temp_throttler, temp_throttler, polarity_throttler, ce_v_max_throttler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_stg_temp_min (split 0)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_stg_temp_max (split 0)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_polarity (split 0)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_ce_v_max (split 0)\n",
      "[INFO] fonduer.utils.udf:57 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88849e9a247d44f1bdf5354da7fb5447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] __main__:5 - PartStgTempMin in split=0: 76751\n",
      "[INFO] __main__:9 - PartStgTempMax in split=0: 76751\n",
      "[INFO] __main__:13 - PartPolarity in split=0: 40838\n",
      "[INFO] __main__:17 - PartCeVMax in split=0: 29297\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_stg_temp_min (split 1)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_stg_temp_max (split 1)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_polarity (split 1)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_ce_v_max (split 1)\n",
      "[INFO] fonduer.utils.udf:57 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f613fa8b6d43b2863aea65ce49b8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] __main__:5 - PartStgTempMin in split=1: 43473\n",
      "[INFO] __main__:9 - PartStgTempMax in split=1: 43473\n",
      "[INFO] __main__:13 - PartPolarity in split=1: 23350\n",
      "[INFO] __main__:17 - PartCeVMax in split=1: 39814\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_stg_temp_min (split 2)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_stg_temp_max (split 2)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_polarity (split 2)\n",
      "[INFO] fonduer.candidates.candidates:125 - Clearing table part_ce_v_max (split 2)\n",
      "[INFO] fonduer.utils.udf:57 - Running UDF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df076145a1b4ee1ad5e056624b0e619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=75), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] __main__:5 - PartStgTempMin in split=2: 8045\n",
      "[INFO] __main__:9 - PartStgTempMax in split=2: 8045\n",
      "[INFO] __main__:13 - PartPolarity in split=2: 5035\n",
      "[INFO] __main__:17 - PartCeVMax in split=2: 9191\n",
      "[INFO] __main__:26 - Total train candidate: 76751\n",
      "[INFO] __main__:27 - Total dev candidate: 43473\n",
      "[INFO] __main__:28 - Total test candidate: 8045\n"
     ]
    }
   ],
   "source": [
    "if FIRST_TIME:\n",
    "    for i, docs in enumerate([train_docs, dev_docs, test_docs]):\n",
    "        candidate_extractor.apply(docs, split=i, parallelism=PARALLEL)\n",
    "        logger.info(\n",
    "            f\"PartStgTempMin in split={i}: \"\n",
    "            f\"{session.query(PartStgTempMin).filter(PartStgTempMin.split == i).count()}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"PartStgTempMax in split={i}: \"\n",
    "            f\"{session.query(PartStgTempMax).filter(PartStgTempMax.split == i).count()}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"PartPolarity in split={i}: \"\n",
    "            f\"{session.query(PartPolarity).filter(PartPolarity.split == i).count()}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"PartCeVMax in split={i}: \"\n",
    "            f\"{session.query(PartCeVMax).filter(PartCeVMax.split == i).count()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "train_cands = candidate_extractor.get_candidates(split = 0)\n",
    "dev_cands = candidate_extractor.get_candidates(split = 1)\n",
    "test_cands = candidate_extractor.get_candidates(split = 2)\n",
    "\n",
    "logger.info(f\"Total train candidate: {len(train_cands[0])}\")\n",
    "logger.info(f\"Total dev candidate: {len(dev_cands[0])}\")\n",
    "logger.info(f\"Total test candidate: {len(test_cands[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multimodal Featurization\n",
    "Unlike dealing with plain unstructured text, `Fonduer` deals with richly formatted data, and consequently featurizes each candidate with a baseline library of multimodal features. \n",
    "\n",
    "### Featurize with `Fonduer`'s optimized Postgres Featurizer\n",
    "We now annotate the candidates in our training, dev, and test sets with features. The `Featurizer` provided by `Fonduer` allows this to be done in parallel to improve performance.\n",
    "\n",
    "View the API provided by the `Featurizer` on [ReadTheDocs](https://fonduer.readthedocs.io/en/latest/user/features.html#fonduer.features.Featurizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.features import Featurizer\n",
    "\n",
    "featurizer = Featurizer(session, [PartStgTempMin, PartStgTempMax, PartPolarity, PartCeVMax])\n",
    "if FIRST_TIME:\n",
    "    %time featurizer.apply(split=0, train=True, parallelism=PARALLEL)\n",
    "    %time featurizer.apply(split=1, parallelism=PARALLEL)\n",
    "    %time featurizer.apply(split=2, parallelism=PARALLEL)\n",
    "\n",
    "%time F_train = featurizer.get_feature_matrices(train_cands)\n",
    "%time F_dev = featurizer.get_feature_matrices(dev_cands)\n",
    "%time F_test = featurizer.get_feature_matrices(test_cands)\n",
    "\n",
    "logger.info(f\"Train shape:\\t{F_train[0].shape}\")\n",
    "logger.info(f\"Test shape:\\t{F_test[0].shape}\")\n",
    "logger.info(f\"Dev shape:\\t{F_dev[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transistor_utils import load_hardware_labels, TRUE, FALSE, ABSTAIN\n",
    "\n",
    "if FIRST_TIME:\n",
    "    load_hardware_labels(session, PartTemp, \"stg_temp_max\" ,annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import Labeler\n",
    "from transistor_lfs import stg_temp_lfs\n",
    "\n",
    "labeler = Labeler(session, [PartTemp])\n",
    "if FIRST_TIME:\n",
    "    %time labeler.apply(split=0, lfs=[stg_temp_lfs], train=True, parallelism=PARALLEL)\n",
    "%time L_train = labeler.get_label_matrices(train_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.supervision import get_gold_labels\n",
    "L_gold_train = get_gold_labels(session, train_cands, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal import analysis\n",
    "\n",
    "analysis.lf_summary(L_train[0], lf_names=labeler.get_keys(), Y=L_gold_train[0].todense().reshape(-1,).tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other. To do so, we use [MeTaL](https://github.com/HazyResearch/metal)'s single-task label model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.label_model import LabelModel\n",
    "\n",
    "gen_model = LabelModel(k=2)\n",
    "%time gen_model.train_model(L_train[0], n_epochs=500, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.predict_proba(L_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals[:, TRUE - 1], bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the learned accuracy parameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_model.weights.lf_accuracy\n",
    "L_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Using the Model to Iterate on Labeling Functions\n",
    "\n",
    "Now that we have learned the generative model, we can stop here and use this to potentially debug and/or improve our labeling function set. First, we apply the LFs to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler.apply(split=1, lfs=[stg_temp_lfs], parallelism=PARALLEL)\n",
    "%time L_dev = labeler.get_label_matrices(dev_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Generative Model Performance\n",
    "\n",
    "At this point, we should be getting an F1 score of around 0.6 to 0.7 on the development set, which is pretty good! However, we should be very careful in interpreting this. Since we developed our labeling functions using this development set as a guide, and our generative model is composed of these labeling functions, we expect it to score very well here!\n",
    "\n",
    "In fact, it is probably somewhat overfit to this set. However this is fine, since in the next, we'll train a more powerful end extraction model which will generalize beyond the development set, and which we will evaluate on a blind test set (i.e. one we never looked at during development).\n",
    "\n",
    "\n",
    "### Training the Discriminative Model\n",
    "\n",
    "Now, we'll use the noisy training labels we generated in the last part to train our end extraction model. For this tutorial, we will be training a simple--but fairly effective--logistic regression model.\n",
    "\n",
    "We use the training marginals to train a discriminative model that classifies each Candidate as a true or false mention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.learning import LogisticRegression\n",
    "\n",
    "disc_model = LogisticRegression()\n",
    "%time disc_model.train((train_cands[0], F_train[0]), train_marginals, n_epochs=50, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transistor_utils import entity_level_f1\n",
    "import pickle\n",
    "pickle_file = 'data/parts_by_doc_dict.pkl'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    parts_by_doc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now, we score using the discriminitive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = disc_model.predict((test_cands[0], F_test[0]), b=0.6, pos_label=TRUE)\n",
    "true_pred = [test_cands[0][_] for _ in np.nditer(np.where(test_score == TRUE))]\n",
    "%time (TP, FP, FN) = entity_level_f1(true_pred, \"stg_temp_max\", test_docs, parts_by_doc=parts_by_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are actually only a few documents that are causing us problems. In particular, we see that `BC546-D` is giving us many false positives. So, let's inspect one of those candidates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonduer.utils.visualizer import Visualizer\n",
    "from transistor_utils import entity_to_candidates\n",
    "vis = Visualizer(\"data/test/pdf\")\n",
    "\n",
    "# Get a list of candidates that match the FN[10] entity\n",
    "fp_cands = entity_to_candidates(FP[1], test_cands[0])\n",
    "# Display a candidate\n",
    "vis.display_candidates([fp_cands[0]])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
